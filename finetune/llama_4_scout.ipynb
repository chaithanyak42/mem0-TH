{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Llama4ForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "model_id = \"unsloth/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "model = Llama4ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Define the End-of-Sequence token\n",
    "EOS_TOKEN = tokenizer.eos_token # Ensure tokenizer is defined in a previous cell\n",
    "\n",
    "# Define the formatting function using list comprehension for conciseness\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Formats the dataset examples into a single text string per example.\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    \n",
    "    # Create the formatted text for each example\n",
    "    texts = [\n",
    "        PROMPT_TEMPLATE.format(instruction, input, output) + EOS_TOKEN\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs)\n",
    "    ]\n",
    "    \n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"/home/ubuntu/mem0/fine-tuning/dataset/memory_dataset_ft.json\" \n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "# The `batched=True` argument processes multiple examples at once for efficiency.\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Print the first 10 instructions from the original dataset (before formatting)\n",
    "# This can be useful for a quick sanity check of the input data.\n",
    "print(\"First 10 instructions from the original dataset:\")\n",
    "print(dataset['instruction'][:10])\n",
    "\n",
    "# Optionally, print the first example of the formatted text to verify formatting\n",
    "# print(\"\\nFirst formatted example:\")\n",
    "# print(dataset['text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first example's instruction and input\n",
    "instruction = dataset['instruction'][0]\n",
    "input_text = dataset['input'][0]\n",
    "\n",
    "# Format the prompt for inference (leave response blank)\n",
    "prompt = PROMPT_TEMPLATE.format(instruction, input_text, \"\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(\n",
    "    [prompt + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode and extract the response\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# Split to get only the model's response part\n",
    "if \"### Response:\" in response:\n",
    "    print(response.split(\"### Response:\")[1].strip())\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=0.2,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"cosmos98a/mem0_llama_4_scout_fine_tuned_f16\", token=\"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\")\n",
    "tokenizer.push_to_hub(\"cosmos98a/mem0_llama_4_scout_fine_tuned_f16\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os # Import os to check path existence\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_name = \"unsloth/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "# --- Find the correct adapter path ---\n",
    "# Option 1: If you know the final checkpoint number (e.g., from training output)\n",
    "lora_adapter_path = \"output/checkpoint-2434\" # <--- CHANGE THIS\n",
    "\n",
    "# Option 2: Find the latest checkpoint directory automatically (more robust)\n",
    "# output_dir = \"output\"\n",
    "# checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "# if not checkpoints:\n",
    "#     raise ValueError(f\"No checkpoint directories found in {output_dir}\")\n",
    "# checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "# lora_adapter_path = checkpoints[-1] # Get the latest checkpoint\n",
    "# print(f\"Found latest checkpoint: {lora_adapter_path}\")\n",
    "\n",
    "\n",
    "# Check if the path and config file exist\n",
    "adapter_config_path = os.path.join(lora_adapter_path, \"adapter_config.json\")\n",
    "if not os.path.exists(adapter_config_path):\n",
    "    raise FileNotFoundError(f\"Cannot find adapter_config.json at {lora_adapter_path}. Please verify the checkpoint path.\")\n",
    "\n",
    "\n",
    "# --- Load Base Model ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "# If using BitsAndBytes quantization for the base model during merge:\n",
    "# bnb_config = BitsAndBytesConfig(...) # Define your config if needed\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_name,\n",
    "#     quantization_config=bnb_config, # Apply quantization if needed\n",
    "#     torch_dtype=torch.bfloat16,     # Use appropriate dtype\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# OR if loading without quantization for merge (requires more memory):\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,  # Load in float16 for merge\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Load LoRA Adapter ---\n",
    "print(f\"Loading LoRA adapter from: {lora_adapter_path}\")\n",
    "model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "print(\"LoRA adapter loaded.\")\n",
    "\n",
    "# --- Merge LoRA and Base Model ---\n",
    "print(\"Merging LoRA weights into the base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"Merge complete.\")\n",
    "\n",
    "# --- Save Merged Model ---\n",
    "merged_model_dir = \"merged_model_fp16\"\n",
    "print(f\"Saving merged model (float16) to: {merged_model_dir}\")\n",
    "model.save_pretrained(merged_model_dir)\n",
    "print(\"Merged model saved.\")\n",
    "\n",
    "# --- Save Tokenizer ---\n",
    "print(f\"Saving tokenizer to: {merged_model_dir}\")\n",
    "# Assuming tokenizer is already loaded correctly from previous cells\n",
    "# If not, load it: tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "print(\"Tokenizer saved.\")\n",
    "\n",
    "print(\"\\n--- Merging and Saving Complete ---\")\n",
    "print(f\"Merged model (float16) is available in the directory: {merged_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Configuration --- (Make sure these are defined or copy them here)\n",
    "base_model_name = \"unsloth/Llama-4-Scout-17B-16E-Instruct\"\n",
    "merged_model_dir = \"merged_model_fp16\"\n",
    "\n",
    "# --- Load and Save Tokenizer ---\n",
    "print(f\"Loading tokenizer for: {base_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "print(f\"Saving tokenizer to: {merged_model_dir}\")\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "print(\"Tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hardcoded HF Token.\n",
      "Loading merged model from: merged_model_fp16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in merged_model_fp16. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- Load the Merged Model and Tokenizer --- # <--- UNCOMMENT THIS BLOCK\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# This step is necessary after a kernel restart\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading merged model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading tokenizer from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mem0/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:531\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    529\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mem0/.venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1149\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1146\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[32m   1147\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern].from_dict(config_dict, **unused_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1150\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1151\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, or contain one of the following strings \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(CONFIG_MAPPING.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1153\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized model in merged_model_fp16. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import os\n",
    "# import torch # Add torch import if missing\n",
    "\n",
    "# # --- Configuration ---\n",
    "# merged_model_dir = \"merged_model_fp16\" # Local directory with the saved merged model\n",
    "# repo_id = \"cosmos98a/mem0_merged_llama4_scout_ft_f16\" # Your target repo on the Hub\n",
    "\n",
    "# # --- Ensure HF Token is Set ---\n",
    "# hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "# if not hf_token:\n",
    "#     # Try getting the token from the specific value set earlier if env var is not set\n",
    "#     try:\n",
    "#         hf_token = \"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\" # The token you set in cell execution_count: 4\n",
    "#         if not hf_token: raise ValueError(\"Token is empty\")\n",
    "#         print(\"Using hardcoded HF Token.\")\n",
    "#     except:\n",
    "#         raise ValueError(\"Hugging Face token not found. Please set HF_TOKEN env var or hardcode it.\")\n",
    "\n",
    "\n",
    "# # --- Load the Merged Model and Tokenizer --- # <--- UNCOMMENT THIS BLOCK\n",
    "# # This step is necessary after a kernel restart\n",
    "# print(f\"Loading merged model from: {merged_model_dir}\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(merged_model_dir, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# print(f\"Loading tokenizer from: {merged_model_dir}\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
    "# print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "\n",
    "# # --- Push to Hub ---\n",
    "# print(f\"\\nPushing merged model and tokenizer to: https://huggingface.co/{repo_id}\")\n",
    "# try:\n",
    "#     # Push the model weights and config\n",
    "#     model.push_to_hub(repo_id, token=hf_token, commit_message=\"Upload merged float16 model\")\n",
    "\n",
    "#     # Push the tokenizer files\n",
    "#     tokenizer.push_to_hub(repo_id, token=hf_token, commit_message=\"Upload tokenizer\")\n",
    "\n",
    "#     print(f\"\\nSuccessfully pushed merged model and tokenizer to: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nAn error occurred during push_to_hub: {e}\")\n",
    "#     print(\"Please ensure:\")\n",
    "#     print(f\"- The repository '{repo_id}' exists on Hugging Face Hub.\")\n",
    "#     print(\"- Your Hugging Face token has write permissions for this repository.\")\n",
    "#     print(\"- You have 'huggingface_hub' installed and potentially 'git-lfs'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_id = \"unsloth/Llama-4-Scout-17B-16E-Instruct\"\n",
    "# !!! IMPORTANT: Verify this is the correct ID for your saved LoRA adapter !!!\n",
    "adapter_id = \"cosmos98a/mem0_llama_4_scout_fine_tuned_f16\"\n",
    "# !!! IMPORTANT: Create a NEW repo on Hugging Face for the merged model !!!\n",
    "# Example: new_repo_id_fp16 = \"cosmos98a/my-scout-merged-fp16\"\n",
    "new_repo_id_fp16 = \"YOUR_NEW_HF_REPO_ID_FOR_MERGED_FP16\" # <--- CHANGE THIS\n",
    "local_save_dir_fp16 = \"./merged_model_fp16_from_hub\" # Local directory to save temporarily\n",
    "\n",
    "# Optional: Set your HF Token if needed (or login via CLI: huggingface-cli login)\n",
    "# hf_token = \"YOUR_HF_WRITE_TOKEN\"\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# --- 1. Load FP16, Apply Adapter, Merge, Save, and Push ---\n",
    "\n",
    "print(f\"--- Scenario 1: Loading {base_model_id} in FP16 ---\")\n",
    "\n",
    "# Load base model in float16\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16, # Use float16 for merging\n",
    "    device_map=\"auto\"          # Use \"auto\" or specific device like \"cuda:0\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer (needed for saving and potentially generation)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "print(f\"--- Applying adapter {adapter_id} to FP16 model ---\")\n",
    "# Load and apply the LoRA adapter\n",
    "# Ensure the adapter exists at the specified 'adapter_id' on the Hub\n",
    "try:\n",
    "    model_fp16 = PeftModel.from_pretrained(model_fp16, adapter_id)\n",
    "    print(\"Adapter applied successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading adapter: {e}\")\n",
    "    print(\"Please ensure the adapter_id is correct and the adapter files exist on the Hub.\")\n",
    "    # Handle error appropriately, maybe exit or skip merging/pushing\n",
    "\n",
    "if hasattr(model_fp16, 'merge_and_unload'):\n",
    "    print(\"--- Merging adapter into FP16 model ---\")\n",
    "    # Merge the adapter weights into the base model\n",
    "    model_fp16 = model_fp16.merge_and_unload()\n",
    "    print(\"Merge complete.\")\n",
    "\n",
    "    print(f\"--- Saving merged FP16 model locally to {local_save_dir_fp16} ---\")\n",
    "    # Save the merged model locally\n",
    "    model_fp16.save_pretrained(local_save_dir_fp16)\n",
    "    tokenizer.save_pretrained(local_save_dir_fp16)\n",
    "    print(\"Merged model and tokenizer saved locally.\")\n",
    "\n",
    "    # --- Push merged FP16 model to Hub ---\n",
    "    print(f\"--- Pushing merged FP16 model to {new_repo_id_fp16} ---\")\n",
    "    # Ensure you have huggingface_hub installed and are logged in (huggingface-cli login)\n",
    "    # Or provide token=hf_token if using an environment variable\n",
    "    try:\n",
    "        model_fp16.push_to_hub(new_repo_id_fp16, commit_message=\"Upload merged FP16 model\")\n",
    "        tokenizer.push_to_hub(new_repo_id_fp16, commit_message=\"Upload tokenizer\")\n",
    "        print(f\"Successfully pushed merged FP16 model and tokenizer to https://huggingface.co/{new_repo_id_fp16}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error pushing merged FP16 model to Hub: {e}\")\n",
    "        print(\"Please ensure:\")\n",
    "        print(f\"- The repository '{new_repo_id_fp16}' exists on Hugging Face Hub.\")\n",
    "        print(\"- Your Hugging Face token has write permissions.\")\n",
    "        print(\"- You have 'huggingface_hub' and 'git-lfs' installed.\")\n",
    "else:\n",
    "    print(\"Model does not have merge_and_unload. Skipping merge, save, and push for FP16.\")\n",
    "\n",
    "\n",
    "# --- 2. Load 4-bit, Apply Adapter (for Inference) ---\n",
    "\n",
    "print(f\"\\n--- Scenario 2: Loading {base_model_id} in 4-bit ---\")\n",
    "\n",
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False, # Set as needed\n",
    "    bnb_4bit_quant_type=\"nf4\",      # Or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Or float16\n",
    ")\n",
    "\n",
    "# Load base model in 4-bit\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True # If required by the base model\n",
    ")\n",
    "\n",
    "# Reload tokenizer if not already loaded\n",
    "if 'tokenizer' not in locals():\n",
    "     tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "print(f\"--- Applying adapter {adapter_id} to 4-bit model ---\")\n",
    "# Load and apply the LoRA adapter\n",
    "# Note: The adapter weights themselves are NOT quantized; they are applied to the quantized base model\n",
    "try:\n",
    "    model_4bit_with_adapter = PeftModel.from_pretrained(model_4bit, adapter_id)\n",
    "    print(\"Adapter applied successfully to 4-bit model.\")\n",
    "    print(\"This model is now ready for inference using the fine-tuned weights on the 4-bit base.\")\n",
    "\n",
    "    # --- Using the 4-bit model with adapter for inference (Example) ---\n",
    "    # print(\"\\n--- Example Inference with 4-bit + Adapter ---\")\n",
    "    # instruction = \"Your instruction here\"\n",
    "    # input_text = \"Your input here\"\n",
    "    # PROMPT_TEMPLATE = \"...\" # Define your prompt template\n",
    "    # prompt = PROMPT_TEMPLATE.format(instruction, input_text, \"\")\n",
    "    # inputs = tokenizer([prompt + tokenizer.eos_token], return_tensors=\"pt\").to(model_4bit_with_adapter.device)\n",
    "    #\n",
    "    # outputs = model_4bit_with_adapter.generate(\n",
    "    #     **inputs,\n",
    "    #     max_new_tokens=200,\n",
    "    #     eos_token_id=tokenizer.eos_token_id,\n",
    "    #     use_cache=True\n",
    "    # )\n",
    "    # response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading adapter for 4-bit model: {e}\")\n",
    "    print(\"Please ensure the adapter_id is correct and the adapter files exist on the Hub.\")\n",
    "\n",
    "\n",
    "# --- Note on Merging/Pushing 4-bit ---\n",
    "# You generally DO NOT merge_and_unload() a 4-bit quantized model with adapters.\n",
    "# The standard practice is to load the base in 4-bit and apply the adapter dynamically for inference,\n",
    "# as shown above (model_4bit_with_adapter). There isn't a standard, widely supported format\n",
    "# for saving a *pre-merged* 4-bit + adapter model for direct loading later.\n",
    "# Therefore, there's no 'push' step included for the 4-bit scenario here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
